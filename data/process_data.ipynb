{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-11-27T14:47:06.825572Z","iopub.status.busy":"2022-11-27T14:47:06.825038Z","iopub.status.idle":"2022-11-27T14:47:07.846872Z","shell.execute_reply":"2022-11-27T14:47:07.845907Z","shell.execute_reply.started":"2022-11-27T14:47:06.82547Z"},"trusted":true},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","from scipy.spatial.distance import cosine\n","import pandas as pd\n","import numpy as np\n","import pickle\n","\n","Course_info = pd.read_csv('./raw/Course_info.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_pickle(file_name):\n","    with open(f'{file_name}', 'rb') as handle:\n","        return pickle.load(handle)\n","    \n","def save_pickle(file, file_name):\n","    with open(f'{file_name}', 'wb') as handle:\n","        pickle.dump(file, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","category_dict = load_pickle('./processed/category.pkl')\n","category_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["course_info = Course_info[Course_info['language'] == 'English']\n","\n","course_info = course_info.drop(['is_paid', 'instructor_url', 'published_time', 'num_reviews', 'num_comments', 'last_update_date', 'language'], axis=1)\n","\n","def convert2dist(interval_list):\n","    closeness_dict = {}\n","    # Function to calculate closeness between two intervals\n","    def calculate_closeness(index_a, index_b):\n","        distance = np.abs(index_a - index_b)\n","        # Using Gaussian-like exponential decay for smoothness\n","        sigma = 2  # Adjust sigma as needed to control the spread of the bell curve\n","        closeness = np.exp(-(distance**2) / (2 * sigma**2))\n","        return closeness\n","\n","    # Create a new dictionary where each key will have an array representing the closeness to each interval\n","    for index_a, interval_a in enumerate(interval_list):\n","        closeness_array = []\n","        for index_b, interval_b in enumerate(interval_list):\n","            closeness = calculate_closeness(index_a, index_b)\n","            closeness_array.append(closeness)\n","        str_interval_a = str(interval_a)\n","        closeness_array = (closeness_array - np.min(closeness_array)) / (np.max(closeness_array) - np.min(closeness_array))\n","        closeness_dict[str_interval_a] = closeness_array\n","    return closeness_dict\n","\n","def quantile_binning(column_name, num_bins=10):\n","    # Equal-frequency binning (Quantile binning)\n","    bins_series = pd.qcut(course_info[column_name], q=num_bins, duplicates='drop')\n","    unique_bins_sorted = bins_series.unique().sort_values()\n","    # series values are strings\n","    bins_series = bins_series.astype(str)\n","    dist = convert2dist(unique_bins_sorted)\n","    pickle_array = [{bin: dist[str(bin)]} for index, bin in enumerate(bins_series.sort_values().unique())]    \n","    save_pickle(pickle_array, f'./processed/bins2{column_name}.pkl')\n","    new_bins_series = bins_series.map(lambda x: dist[str(x)])\n","    course_info[f\"{column_name}_dist\"] = new_bins_series\n","    # course_info.drop(column_name, axis=1, inplace=True)\n","    \n","quantile_binning('price', num_bins=10)\n","quantile_binning('num_lectures', num_bins=10)\n","quantile_binning('content_length_min', num_bins=10)\n","course_info"]},{"cell_type":"markdown","metadata":{},"source":["# Convert to Numerical Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def process_and_save_category_data(df, column_name, pickle_file):\n","    data_dict = df.groupby([column_name]).size().sort_values(ascending=False).to_dict()\n","    # Create an enumerated dictionary with counts\n","    data_dict = {key: idx for idx, key in enumerate(data_dict)}\n","    \n","    # id2name dictionary\n","    id2name = [{'id': v, 'name': k } for k, v in data_dict.items()]\n","\n","    # Serialize the dictionary using pickle\n","    with open(f'./processed/{pickle_file}', 'wb') as handle:\n","        pickle.dump(id2name, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    \n","    return data_dict\n","\n","\n","# Apply the function to each category and save the mappings as pickle files\n","category = process_and_save_category_data(course_info, 'category', 'category.pkl')\n","subcategory = process_and_save_category_data(course_info, 'subcategory', 'subcategory.pkl')\n","topic = process_and_save_category_data(course_info, 'topic', 'topic.pkl')\n","instructor = process_and_save_category_data(course_info, 'instructor_name', 'instructor.pkl')\n","\n","# map the category data to the original dataframe\n","course_info['category_id'] = course_info['category'].map(category)\n","course_info['subcategory_id'] = course_info['subcategory'].map(subcategory)\n","course_info['topic_id'] = course_info['topic'].map(topic)\n","course_info['instructor_name_id'] = course_info['instructor_name'].map(instructor)\n","\n","course_info"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["files = ['./processed/category.pkl', './processed/content_length_min.pkl', './processed/instructor.pkl', './processed/num_lecture.pkl', './processed/num_lectures.pkl', './processed/price.pkl', './processed/subcategory.pkl', './processed/topic.pkl']\n","# Function to load a pickle file\n","def load_pickle(file_name):\n","    with open(f'{file_name}', 'rb') as handle:\n","        return pickle.load(handle)\n","\n","# Load all the pickle files\n","category = load_pickle(files[0])\n","subcategory = load_pickle(files[6])\n","topic = load_pickle(files[7])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","# Load the pre-trained SentenceTransformer model (do this outside the function if possible)\n","model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n","\n","def generate_similarity_matrix(name, categories):\n","    # Extract category names and encode them in a batch\n","    category_names = [category['name'] for category in categories]\n","    category_embeddings = model.encode(category_names)\n","\n","    # Compute the cosine similarity matrix in a vectorized way\n","    similarity_matrix = cosine_similarity(category_embeddings)\n","    similarity_matrix = (similarity_matrix - similarity_matrix.min()) / (similarity_matrix.max() - similarity_matrix.min())\n","    # Save the similarity matrix to a file\n","    with open(f'./processed/{name}_similarity_matrix.npy', 'wb') as f:\n","        np.save(f, similarity_matrix)\n","\n","    return similarity_matrix\n","\n","# Generate similarity matrices\n","category_similarity_matrix = generate_similarity_matrix('category', category)\n","subcategory_similarity_matrix = generate_similarity_matrix('subcategory', subcategory)\n","topic_similarity_matrix = generate_similarity_matrix('topic', topic)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# category_similarity_matrix, subcategory_similarity_matrix, topic_similarity_matrix\n","def map_topic(x, topic_similarity_matrix):\n","    try:\n","        return topic_similarity_matrix[int(x)]\n","    except:\n","        return np.zeros(topic_similarity_matrix.shape[0]) \n","    \n","# course_info['category_dist'] = course_info['category_id'].apply(lambda x: category_similarity_matrix[x])\n","# course_info['subcategory_dist'] = course_info['subcategory_id'].apply(lambda x: subcategory_similarity_matrix[x])\n","# course_info['topic_dist'] = course_info['topic_id'].apply(lambda x: map_topic(x, topic_similarity_matrix))\n","\n","def category_dist(x):\n","    array = np.zeros(category_similarity_matrix.shape[0])\n","    array[x] = 1\n","    return array\n","    \n","def subcategory_dist(x):\n","    array = np.zeros(subcategory_similarity_matrix.shape[0])\n","    array[x] = 1\n","    return array\n","\n","\n","course_info['category_dist'] = course_info['category_id'].apply(lambda x: category_dist(x))\n","course_info['subcategory_dist'] = course_info['subcategory_id'].apply(lambda x: subcategory_dist(x))\n","course_info['topic_dist'] = course_info['topic_id'].apply(lambda x: map_topic(x, topic_similarity_matrix))\n","\n","course_info"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def concatenate_profile(row):\n","    return np.concatenate([\n","        row['category_dist'],\n","        row['subcategory_dist'],\n","        row['price_dist'],\n","        row['num_lectures_dist'],\n","        row['content_length_min_dist'],\n","    ])\n","\n","# Apply this function across each row\n","course_info['profile'] = course_info.apply(concatenate_profile, axis=1)\n","\n","# Create a minified version of the DataFrame\n","course_info_minified = course_info[['id', 'title', 'topic', 'avg_rating', 'course_url', 'num_lectures', 'price', 'profile']]\n","course_info_minified"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["course_info_minified.to_pickle('./processed/course_info_minified_v2.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2658012,"sourceId":4553658,"sourceType":"datasetVersion"}],"dockerImageVersionId":30301,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
